{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Safety vest detection via person detection + color detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pasha/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/pasha/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/pasha/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/pasha/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/pasha/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/pasha/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/pasha/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/pasha/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/pasha/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/pasha/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/pasha/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/pasha/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import six.moves.urllib as urllib\n",
    "import sys\n",
    "import tarfile\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "import cv2\n",
    "import time\n",
    "\n",
    "from collections import defaultdict\n",
    "from io import StringIO\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth=True\n",
    " \n",
    "class ObjectDetector:\n",
    "    def __init__(self):\n",
    "        # Models can bee found here: https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/detection_model_zoo.md\n",
    "        MODEL_NAME = 'ssd_inception_v2_coco_2017_11_17'\n",
    "#         MODEL_NAME = 'faster_rcnn_resnet101_lowproposals_coco_2018_01_28'\n",
    "        # MODEL_NAME = 'faster_rcnn_nas_lowproposals_coco_2018_01_28'\n",
    "        MODEL_FILE = MODEL_NAME + '.tar.gz'\n",
    "        DOWNLOAD_BASE = 'http://download.tensorflow.org/models/object_detection/'\n",
    "\n",
    "        PATH_TO_CKPT = MODEL_NAME + '/frozen_inference_graph.pb'\n",
    "\n",
    "        PATH_TO_LABELS = os.path.join('data', 'mscoco_label_map.pbtxt')\n",
    "\n",
    "        NUM_CLASSES = 90\n",
    "\n",
    "        # Download Model\n",
    "        if not os.path.isfile(PATH_TO_CKPT):\n",
    "            opener = urllib.request.URLopener()\n",
    "            opener.retrieve(DOWNLOAD_BASE + MODEL_FILE, MODEL_FILE)\n",
    "            tar_file = tarfile.open(MODEL_FILE)\n",
    "            for file in tar_file.getmembers():\n",
    "                file_name = os.path.basename(file.name)\n",
    "                if 'frozen_inference_graph.pb' in file_name:\n",
    "                    tar_file.extract(file, os.getcwd())\n",
    "\n",
    "        self.detection_graph = tf.Graph()\n",
    "        with self.detection_graph.as_default():\n",
    "            od_graph_def = tf.GraphDef()\n",
    "            with tf.gfile.GFile(PATH_TO_CKPT, 'rb') as fid:\n",
    "                serialized_graph = fid.read()\n",
    "                od_graph_def.ParseFromString(serialized_graph)\n",
    "                tf.import_graph_def(od_graph_def, name='')\n",
    "\n",
    "        self.threshold = 0.7\n",
    "\n",
    "        self.sess = tf.Session(config=config, graph=self.detection_graph)\n",
    "        \n",
    "\n",
    "    def findperson(self, image_np):\n",
    "        with self.sess.graph.as_default():\n",
    "            image_np_expanded = np.expand_dims(image_np, axis=0)\n",
    "\n",
    "            image_tensor = self.detection_graph.get_tensor_by_name('image_tensor:0')\n",
    "            boxes = self.detection_graph.get_tensor_by_name('detection_boxes:0')\n",
    "            scores = self.detection_graph.get_tensor_by_name('detection_scores:0')\n",
    "            classes = self.detection_graph.get_tensor_by_name('detection_classes:0')\n",
    "            num_detections = self.detection_graph.get_tensor_by_name('num_detections:0')\n",
    "            \n",
    "            start_time = time.time()\n",
    "            (boxes, scores, classes, num_detections) = self.sess.run(\n",
    "                [boxes, scores, classes, num_detections],\n",
    "                feed_dict={image_tensor: image_np_expanded})\n",
    "            print(\"tf obj_det predict time: \", time.time() - start_time)\n",
    "\n",
    "            personboxes = []\n",
    "            for pred_idx in range(scores.shape[1]):\n",
    "                if scores[0, pred_idx] > self.threshold:\n",
    "                    if classes[0, pred_idx] == 1:\n",
    "                        personboxes.append([classes[0, pred_idx], scores[0, pred_idx], boxes[0, pred_idx, 1], boxes[0, pred_idx, 0], boxes[0, pred_idx, 3], boxes[0, pred_idx, 2]])\n",
    "                else:\n",
    "                    break\n",
    "\n",
    "            return personboxes\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf obj_det predict time:  3.617732286453247\n",
      "frame processing time:  3.634870767593384\n",
      "tf obj_det predict time:  0.06602311134338379\n",
      "frame processing time:  0.07580089569091797\n",
      "tf obj_det predict time:  0.051958322525024414\n",
      "frame processing time:  0.06017875671386719\n",
      "tf obj_det predict time:  0.052850961685180664\n",
      "frame processing time:  0.05946850776672363\n",
      "tf obj_det predict time:  0.04598689079284668\n",
      "frame processing time:  0.051323890686035156\n",
      "tf obj_det predict time:  0.05102682113647461\n",
      "frame processing time:  0.05657339096069336\n",
      "tf obj_det predict time:  0.04952287673950195\n",
      "frame processing time:  0.05483651161193848\n",
      "tf obj_det predict time:  0.053545475006103516\n",
      "frame processing time:  0.05910944938659668\n",
      "tf obj_det predict time:  0.05388951301574707\n",
      "frame processing time:  0.05917096138000488\n",
      "tf obj_det predict time:  0.0531158447265625\n",
      "frame processing time:  0.058679819107055664\n",
      "tf obj_det predict time:  0.049602508544921875\n",
      "frame processing time:  0.05502486228942871\n",
      "tf obj_det predict time:  0.05202460289001465\n",
      "frame processing time:  0.057019710540771484\n",
      "tf obj_det predict time:  0.05416440963745117\n",
      "frame processing time:  0.060712337493896484\n",
      "tf obj_det predict time:  0.051492929458618164\n",
      "frame processing time:  0.05722975730895996\n",
      "tf obj_det predict time:  0.05234789848327637\n",
      "frame processing time:  0.057851314544677734\n",
      "tf obj_det predict time:  0.049970388412475586\n",
      "frame processing time:  0.055330514907836914\n",
      "tf obj_det predict time:  0.0534052848815918\n",
      "frame processing time:  0.05863547325134277\n",
      "tf obj_det predict time:  0.053502798080444336\n",
      "frame processing time:  0.05902290344238281\n",
      "tf obj_det predict time:  0.053772926330566406\n",
      "frame processing time:  0.05904817581176758\n",
      "tf obj_det predict time:  0.04897332191467285\n",
      "frame processing time:  0.05416226387023926\n",
      "tf obj_det predict time:  0.0510401725769043\n",
      "frame processing time:  0.05613970756530762\n",
      "tf obj_det predict time:  0.05173516273498535\n",
      "frame processing time:  0.05698728561401367\n",
      "tf obj_det predict time:  0.05322575569152832\n",
      "frame processing time:  0.05857539176940918\n",
      "tf obj_det predict time:  0.0509943962097168\n",
      "frame processing time:  0.05641603469848633\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import sys, os\n",
    "import time\n",
    " \n",
    "\n",
    "#function takes image of detected person and using color detection finds safety vest\n",
    "def detect_vest(cv2Image):      #cv2Image - image of detected person\n",
    "    #convertation to hsv palette, format - (Hue, Saturation, Value)\n",
    "    hsv = cv2.cvtColor(cv2Image, cv2.COLOR_BGR2HSV) \n",
    "    lower_range = np.array([0, int(35/100*255), int(25/100*255)]) # lower range of safety vest color\n",
    "    upper_range = np.array([80, 255, 255]) # upper range of safety vest color\n",
    "    \n",
    "    mask = cv2.inRange(hsv, lower_range, upper_range) #filter the area of needed color\n",
    "    \n",
    "    image_width = hsv.shape[1]\n",
    "    image_height = hsv.shape[0]\n",
    "    \n",
    "    kernel_e = np.ones((image_height//20,image_height//20),np.uint8)  \n",
    "    kernel_d = np.ones((image_height//10,image_height//10),np.uint8)  \n",
    "    # eroding an image to remove small areas\n",
    "    erosion = cv2.erode(mask,kernel_e,iterations = 1)\n",
    "    #dilation will renew the size of areas, that left after erosion\n",
    "    dilation = cv2.dilate(erosion,kernel_d,iterations = 1)\n",
    "    detected=0\n",
    "    x,y,w,h = 0,0,0,0\n",
    "    if len(dilation[dilation>0])!=0:\n",
    "        detected=1\n",
    "        ret, thresh = cv2.threshold(dilation, 127, 255, 0)\n",
    "        contours, hierarchy = cv2.findContours(thresh, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "        areas = [cv2.contourArea(c) for c in contours]\n",
    "        max_index = np.argmax(areas)\n",
    "        cnt=contours[max_index]\n",
    "        area = cv2.contourArea(cnt)\n",
    "        x,y,w,h = cv2.boundingRect(cnt)\n",
    "    return x,y,x+w,y+h, detected\n",
    "    \n",
    "    \n",
    "\n",
    "obj_detector = ObjectDetector()\n",
    "\n",
    "# capture video\n",
    "if(len(sys.argv) > 1):\n",
    "    video_path = sys.argv[1]\n",
    "video_path = \"./1.mp4\"\n",
    "\n",
    "\n",
    "try:\n",
    "    video_capture = cv2.VideoCapture(video_path)\n",
    "except:\n",
    "    video_capture = cv2.VideoCapture(0)\n",
    "\n",
    " \n",
    "ret, frame = video_capture.read()\n",
    "fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "out_cap = cv2.VideoWriter(video_path[:-4] + \"_res_nobb.avi\", fourcc, 20.0, (int(frame.shape[1] ), int(frame.shape[0])))\n",
    "\n",
    "person_boxes = []               # for storing predictions from object detector\n",
    " \n",
    "xmin_v, ymin_v, xmax_v, ymax_v = 0,0,0,0\n",
    "xmin, ymin, xmax, ymax = 0,0,0,0\n",
    "detected = 0\n",
    "shot_time_start = time.time()\n",
    "shot_time = time.time()\n",
    "\n",
    "while True:\n",
    "    \n",
    "    shot_time_start = time.time()\n",
    "\n",
    "    ret, frame = video_capture.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    person_boxes = obj_detector.findperson(frame)\n",
    "    for box in person_boxes:\n",
    "        xmin = int(box[2] * frame.shape[1])\n",
    "        ymin = int(box[3] * frame.shape[0])\n",
    "        xmax = int(box[4] * frame.shape[1])\n",
    "        ymax = int(box[5] * frame.shape[0])\n",
    "\n",
    "        cv2.rectangle(frame, (int(xmin), int(ymin)), (int(xmax), int(ymax)), (0, 255, 0))\n",
    "        #detect vest\n",
    "        xmin_v, ymin_v, xmax_v, ymax_v, detected = detect_vest(frame[ymin:ymax,xmin:xmax])\n",
    "        #if vest detected - then draw it\n",
    "        if detected:\n",
    "            xmin_v, ymin_v, xmax_v, ymax_v = xmin_v+xmin, ymin_v+ymin, xmax_v+xmin, ymax_v+ymin\n",
    "            cv2.rectangle(frame, (int(xmin_v), int(ymin_v)), (int(xmax_v), int(ymax_v)), (255, 255, 0))\n",
    "\n",
    "\n",
    "    shot_time = time.time() - shot_time_start\n",
    "    fps = int(1/shot_time)\n",
    "    cv2.putText(frame,\"FPS: \"+str(fps), (30,50), cv2.FONT_HERSHEY_SIMPLEX, 2, (0,255,255))\n",
    "    out_cap.write(frame)\n",
    "    cv2.imshow('Video', frame)\n",
    "    print(\"frame processing time: \", time.time() - shot_time_start)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "out_cap.release()\n",
    "video_capture.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
